{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"D:/main project/videoplayback1.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, frame1 = cap.read()\n",
    "ret, frame2 = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:650: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'cv::arithm_op'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m----> 2\u001b[0m     diff \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabsdiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe2\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      3\u001b[0m     gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(diff, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY) \n\u001b[0;32m      4\u001b[0m     blur \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mGaussianBlur(gray, (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m), \u001b[38;5;241m0\u001b[39m) \n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:650: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'cv::arithm_op'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "while cap.isOpened():\n",
    "    diff = cv2.absdiff(frame1, frame2) \n",
    "    gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY) \n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0) \n",
    "    _, thresh = cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY) \n",
    "    dilated = cv2.dilate(thresh, None, iterations=3) \n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) \n",
    "\n",
    "    for contour in contours:\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "\n",
    "        if cv2.contourArea(contour) < 900:\n",
    "            continue\n",
    "\n",
    "        cv2.rectangle(frame1, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Frame Difference', frame1) \n",
    "    frame1 = frame2\n",
    "    ret, frame2 = cap.read()\n",
    "\n",
    "    if cv2.waitKey(40) == 27: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a background subtractor object\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2() \n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(\"D:/main project/videoplayback.mp4\")\n",
    "\n",
    "while(1):\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Apply background subtraction\n",
    "    fgmask = fgbg.apply(frame) \n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',fgmask)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def keep_subject_visible(video_path):\n",
    "  \"\"\"\n",
    "  Performs frame subtraction on a video and attempts to keep the subject visible.\n",
    "\n",
    "  Args:\n",
    "    video_path: Path to the input video file.\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "\n",
    "  cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "  # Get initial frame for background estimation\n",
    "  ret, prev_frame = cap.read()\n",
    "  gray_prev = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  while(True):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "      break\n",
    "\n",
    "    gray_curr = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate frame difference\n",
    "    frame_diff = cv2.absdiff(gray_prev, gray_curr)\n",
    "\n",
    "    # Threshold the difference image to get a binary mask\n",
    "    _, thresh = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply morphological operations to clean the mask (optional)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Invert the mask to get the subject\n",
    "    #mask = cv2.bitwise_not(thresh)\n",
    "\n",
    "    # Apply the mask to the current frame\n",
    "    #result = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "    result = cv2.bitwise_and(frame, frame, mask=thresh)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow('Subject', result)\n",
    "\n",
    "    # Update previous frame\n",
    "    gray_prev = gray_curr\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "      break\n",
    "\n",
    "  cap.release()\n",
    "  cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage:\n",
    "video_path = \"D:/main project/videoplayback.mp4\"\n",
    "keep_subject_visible(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def isolate_subject(video_path):\n",
    "  cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "  while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "      break\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply thresholding (replace with an appropriate thresholding method)\n",
    "    thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # Inpaint the background (replace with an inpainting library/function)\n",
    "    inpainted_frame = cv2.inpaint(frame, thresh, 3, cv2.INPAINT_TELEA)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow('Subject', inpainted_frame)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "      break\n",
    "\n",
    "  cap.release()\n",
    "  cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage:\n",
    "video_path = \"D:/main project/videoplayback.mp4\"\n",
    "isolate_subject(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def keep_subject_visible_mog2(video_path):\n",
    "  \"\"\"\n",
    "  Performs frame subtraction with MOG2 and attempts to keep the subject visible.\n",
    "\n",
    "  Args:\n",
    "    video_path: Path to the input video file.\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "\n",
    "  cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "  # Create MOG2 background subtractor\n",
    "  fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "  # Read first frame for initialization\n",
    "  ret, frame = cap.read()\n",
    "  if not ret:\n",
    "    print(\"Error: Could not read first frame.\")\n",
    "    return\n",
    "\n",
    "  # Initialize background model\n",
    "  fgmask = fgbg.apply(frame)\n",
    "\n",
    "  while(True):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "      break\n",
    "\n",
    "    # Update background model\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Apply thresholding to foreground mask (optional)\n",
    "    _, thresh = cv2.threshold(fgmask, 25, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply morphological operations to refine mask (optional)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Invert the mask to isolate the subject (optional)\n",
    "    # mask = cv2.bitwise_not(thresh)\n",
    "\n",
    "    # Apply the mask to the current frame (consider both options)\n",
    "    # Option 1: Mask out background (might not preserve subject color well)\n",
    "    # result = cv2.bitwise_and(frame, frame, mask=thresh)\n",
    "    # Option 2: Keep subject color, inpaint background (might require additional steps)\n",
    "    result = frame.copy()\n",
    "    result[thresh == 0] = (0, 0, 0)  # Set background pixels to black\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow('Subject', result)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "      break\n",
    "\n",
    "  cap.release()\n",
    "  cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage:\n",
    "video_path = \"D:/main project/videoplayback.mp4\"\n",
    "keep_subject_visible_mog2(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: Invalid crop dimensions. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n",
      "Warning: No subject found in frame. Using original crop region.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def keep_subject_visible_and_crop(video_path, output_video_path, crop_x, crop_y, crop_width, crop_height):\n",
    "  \"\"\"\n",
    "  Crops a video while attempting to keep the subject visible using frame subtraction.\n",
    "\n",
    "  Args:\n",
    "    video_path: Path to the input video file.\n",
    "    output_video_path: Path to save the cropped video.\n",
    "    crop_x: x-coordinate of the top-left corner of the cropping region.\n",
    "    crop_y: y-coordinate of the top-left corner of the cropping region.\n",
    "    crop_width: Width of the cropping region.\n",
    "    crop_height: Height of the cropping region.\n",
    "  \"\"\"\n",
    "\n",
    "  cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "  # Check if video opened successfully\n",
    "  if not cap.isOpened():\n",
    "    print(\"Error opening video:\", video_path)\n",
    "    return\n",
    "\n",
    "  # Get video properties\n",
    "  frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "  frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "  # Validate crop region within frame dimensions\n",
    "  if crop_x + crop_width > frame_width or crop_y + crop_height > frame_height:\n",
    "    print(\"Error: Crop region exceeds frame boundaries. Adjust coordinates.\")\n",
    "    return\n",
    "\n",
    "  # Define video writer for output\n",
    "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "  out = cv2.VideoWriter(output_video_path, fourcc, fps, (crop_width, crop_height))\n",
    "\n",
    "  # Create MOG2 background subtractor\n",
    "  fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "  # Read first frame for initialization\n",
    "  ret, frame = cap.read()\n",
    "  if not ret:\n",
    "    print(\"Error: Could not read first frame.\")\n",
    "    return\n",
    "\n",
    "  # Initialize background model\n",
    "  fgmask = fgbg.apply(frame)\n",
    "\n",
    "  while(True):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "      break\n",
    "\n",
    "    # Update background model\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Apply thresholding to foreground mask (optional)\n",
    "    _, thresh = cv2.threshold(fgmask, 25, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply morphological operations to refine mask (optional)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Find contours in the foreground mask (assuming subject is a single object)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Check if a contour is found\n",
    "    if len(contours) > 0:\n",
    "      # Get the largest contour (assuming the subject is the largest object)\n",
    "      largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "      # Get bounding rectangle of the largest contour\n",
    "      x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "      # Consider intersection of subject bounding box and crop region\n",
    "      crop_x1 = max(crop_x, x)  # Adjust crop_x if subject is partially outside\n",
    "      crop_y1 = max(crop_y, y)  # Adjust crop_y if subject is partially outside\n",
    "      crop_width1 = min(crop_width, x + w - crop_x1)  # Adjust crop width if subject is smaller\n",
    "      crop_height1 = min(crop_height, y + h - crop_y1)  # Adjust crop height if subject is smaller\n",
    "\n",
    "      # Ensure crop dimensions are valid\n",
    "      if crop_width1 <= 0 or crop_height1 <= 0:\n",
    "        print(\"Warning: Invalid crop dimensions. Using original crop region.\")\n",
    "        cropped_frame = frame[crop_y:crop_y+crop_height, crop_x:crop_x+crop_width]\n",
    "      else:\n",
    "        cropped_frame = frame[crop_y1:crop_y1+crop_height1, crop_x1:crop_x1+crop_width1]\n",
    "\n",
    "    else:\n",
    "      # Handle case when no subject is found\n",
    "      print(\"Warning: No subject found in frame. Using original crop region.\")\n",
    "      cropped_frame = frame[crop_y:crop_y+crop_height, crop_x:crop_x+crop_width]\n",
    "\n",
    "    # Check if cropped_frame is valid before resizing\n",
    "    if cropped_frame is not None and cropped_frame.size > 0:\n",
    "      resized_frame = cv2.resize(cropped_frame, (crop_width, crop_height))\n",
    "      out.write(resized_frame)\n",
    "    else:\n",
    "      print(\"Warning: Skipping frame due to empty cropped_frame.\")\n",
    "\n",
    "  cap.release()\n",
    "  out.release()\n",
    "  cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage:\n",
    "input_video = \"D:/main project/videoplayback.mp4\"\n",
    "output_video = \"cropped_output.mp4\"\n",
    "crop_x = 100  # Adjust these values as needed\n",
    "crop_y = 50\n",
    "crop_width = 640\n",
    "crop_height = 360\n",
    "\n",
    "keep_subject_visible_and_crop(input_video, output_video, crop_x, crop_y, crop_width, crop_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vpi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvpi\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vpi'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
